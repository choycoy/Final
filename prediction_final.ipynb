{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4+YHMIn51lhI4RVtiWj8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choycoy/Final/blob/main/prediction_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ljJz3F-CSUp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q_df = pd.read_csv(\"/content/drive/My Drive/final/questions.csv\",sep=\",\")\n",
        "q_df['question_id'] = q_df['question_id'].str.replace('q', '')\n",
        "q_df"
      ],
      "metadata": {
        "id": "8vJlKMKgCXMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__1.csv\",sep=\",\")\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__2.csv\",sep=\",\")\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__3.csv\",sep=\",\")\n",
        "df4 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__4.csv\",sep=\",\")\n",
        "df5 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__5.csv\",sep=\",\")\n",
        "df6 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__6.csv\",sep=\",\")\n",
        "df7 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__7.csv\",sep=\",\")\n",
        "df8 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__8.csv\",sep=\",\")\n",
        "df9 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__9.csv\",sep=\",\")\n",
        "df10 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__10.csv\",sep=\",\")\n",
        "df11 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__11.csv\",sep=\",\")\n",
        "df12 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__12.csv\",sep=\",\")\n",
        "df13 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__13.csv\",sep=\",\")\n",
        "df14 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__14.csv\",sep=\",\")\n",
        "df15 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__15.csv\",sep=\",\")\n",
        "df16 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__16.csv\",sep=\",\")\n",
        "df17 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__17.csv\",sep=\",\")\n",
        "df18 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__18.csv\",sep=\",\")\n",
        "df19 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__19.csv\",sep=\",\")\n",
        "df20 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__20.csv\",sep=\",\")\n",
        "df21 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__21.csv\",sep=\",\")\n",
        "df22 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__22.csv\",sep=\",\")\n",
        "df23 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__23.csv\",sep=\",\")\n",
        "df24 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__24.csv\",sep=\",\")\n",
        "df25 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__25.csv\",sep=\",\")\n",
        "df26 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__26.csv\",sep=\",\")\n",
        "df27 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__27.csv\",sep=\",\")\n",
        "df28 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__28.csv\",sep=\",\")\n",
        "\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8,df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20,df21, df22,df23, df24, df25, df26, df27, df28])"
      ],
      "metadata": {
        "id": "IKjnuTryCZLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['question_id'] = df['question_id'].astype('int64')\n",
        "q_df['question_id'] = q_df['question_id'].astype('int64')\n",
        "\n",
        "# Merge the dataframes on 'question_id'\n",
        "merged_df = pd.merge(df, q_df[['question_id', 'correct_answer']], on='question_id', how='left')\n",
        "\n",
        "# Create the 'is_correct' column based on the comparison between 'user_answer' and 'correct_answer'\n",
        "merged_df['is_correct'] = (merged_df['user_answer'] == merged_df['correct_answer']).astype(int)\n",
        "\n",
        "# Now merged_df will have a new column 'is_correct' indicating if the user_answer is correct or not\n",
        "print(merged_df)"
      ],
      "metadata": {
        "id": "uCqivkQKCa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataframe by timestamp to ensure the data is in chronological order\n",
        "merged_df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
        "\n",
        "# Calculate Correctness Rate and Interaction Count for each user\n",
        "merged_df['correct_count'] = merged_df.groupby('user_id')['is_correct'].cumsum()  # Cumulative sum of correct answers for each user\n",
        "print(merged_df['correct_count'])\n",
        "\n",
        "merged_df['interaction_count'] = merged_df.groupby('user_id').cumcount() + 1  # Cumulative count of interactions for each user\n",
        "print(merged_df['interaction_count'])\n",
        "\n",
        "merged_df['correctness_rate'] = merged_df['correct_count'] / merged_df['interaction_count']  # Calculate the correctness rate for each interaction\n",
        "merged_df['average_elapsed_time'] = merged_df.groupby('user_id')['elapsed_time'].transform('mean')\n",
        "\n",
        "# Drop unnecessary columns if needed\n",
        "merged_df.drop(['correct_count', 'interaction_count'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "oGldVC_zCchM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['next_is_correct'] = merged_df['is_correct'].shift(-1)\n",
        "merged_df.dropna(subset=['next_is_correct'], inplace=True)"
      ],
      "metadata": {
        "id": "W5MCnhCsCeJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust features = ['average_elapsed_time', 'correctness_rate'] or features = ['correctness_rate']\n",
        "features = ['average_elapsed_time', 'correctness_rate']"
      ],
      "metadata": {
        "id": "e_tE-RcfCelC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = merged_df[features]\n",
        "y = merged_df['next_is_correct']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "-ViNs-vaCgH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Recall: {:.2f}\".format(recall))\n",
        "print(\"F1-score: {:.2f}\".format(f1))"
      ],
      "metadata": {
        "id": "MpuNWmIhChNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "KMosmIF8CmbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "bag = BaggingClassifier(base_estimator=model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = bag.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Recall: {:.2f}\".format(recall))\n",
        "print(\"F1-score: {:.2f}\".format(f1))"
      ],
      "metadata": {
        "id": "ckYUKwqACv22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "classifier_boost = AdaBoostClassifier(base_estimator=model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "classifier_boost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = classifier_boost.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Recall: {:.2f}\".format(recall))\n",
        "print(\"F1-score: {:.2f}\".format(f1))"
      ],
      "metadata": {
        "id": "deCbPUU8CzrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
        "print(\"Precision: {:.2f}\".format(precision))\n",
        "print(\"Recall: {:.2f}\".format(recall))\n",
        "print(\"F1-score: {:.2f}\".format(f1))\n"
      ],
      "metadata": {
        "id": "WKDU952qCxDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}