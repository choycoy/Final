{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc9zGuChgRnuenciMoQXS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choycoy/Final/blob/main/recommendation_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVyRNP0sDb-e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q_df = pd.read_csv(\"/content/drive/My Drive/final/questions.csv\",sep=\",\")"
      ],
      "metadata": {
        "id": "mgd-YqbLDtZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_df['question_id'] = q_df['question_id'].str.replace('q', '')\n",
        "q_df"
      ],
      "metadata": {
        "id": "QB6rcJb0Dvot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__1.csv\",sep=\",\")\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__2.csv\",sep=\",\")\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__3.csv\",sep=\",\")\n",
        "df4 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__4.csv\",sep=\",\")\n",
        "df5 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__5.csv\",sep=\",\")\n",
        "df6 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__6.csv\",sep=\",\")\n",
        "df7 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__7.csv\",sep=\",\")\n",
        "df8 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__8.csv\",sep=\",\")\n",
        "df9 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__9.csv\",sep=\",\")\n",
        "df10 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__10.csv\",sep=\",\")\n",
        "df11 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__11.csv\",sep=\",\")\n",
        "df12 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__12.csv\",sep=\",\")\n",
        "df13 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__13.csv\",sep=\",\")\n",
        "df14 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__14.csv\",sep=\",\")\n",
        "df15 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__15.csv\",sep=\",\")\n",
        "df16 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__16.csv\",sep=\",\")\n",
        "df17 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__17.csv\",sep=\",\")\n",
        "df18 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__18.csv\",sep=\",\")\n",
        "df19 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__19.csv\",sep=\",\")\n",
        "df20 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__20.csv\",sep=\",\")\n",
        "df21 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__21.csv\",sep=\",\")\n",
        "df22 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__22.csv\",sep=\",\")\n",
        "df23 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__23.csv\",sep=\",\")\n",
        "df24 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__24.csv\",sep=\",\")\n",
        "df25 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__25.csv\",sep=\",\")\n",
        "df26 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__26.csv\",sep=\",\")\n",
        "df27 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__27.csv\",sep=\",\")\n",
        "df28 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__28.csv\",sep=\",\")\n",
        "\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8,df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20,df21, df22,df23, df24, df25, df26, df27, df28])"
      ],
      "metadata": {
        "id": "0QGaH951DwvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'question_id' column in df1 and df2 to the same data type (e.g., int64)\n",
        "df['question_id'] = df['question_id'].astype('int64')\n",
        "q_df['question_id'] = q_df['question_id'].astype('int64')\n",
        "\n",
        "# Merge the dataframes on 'question_id'\n",
        "merged_df = pd.merge(df, q_df[['question_id', 'correct_answer']], on='question_id', how='left')\n",
        "\n",
        "# Create the 'is_correct' column based on the comparison between 'user_answer' and 'correct_answer'\n",
        "merged_df['is_correct'] = (merged_df['user_answer'] == merged_df['correct_answer']).astype(int)\n",
        "\n",
        "# Now merged_df will have a new column 'is_correct' indicating if the user_answer is correct or not\n",
        "print(merged_df)"
      ],
      "metadata": {
        "id": "ubMIMvrlDyQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataframe by timestamp to ensure the data is in chronological order\n",
        "merged_df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
        "\n",
        "# Calculate Correctness Rate and Interaction Count for each user\n",
        "merged_df['correct_count'] = merged_df.groupby('user_id')['is_correct'].cumsum()  # Cumulative sum of correct answers for each user\n",
        "print(merged_df['correct_count'])\n",
        "\n",
        "merged_df['interaction_count'] = merged_df.groupby('user_id').cumcount() + 1  # Cumulative count of interactions for each user\n",
        "print(merged_df['interaction_count'])\n",
        "\n",
        "merged_df['correctness_rate'] = merged_df['correct_count'] / merged_df['interaction_count']  # Calculate the correctness rate for each interaction\n",
        "merged_df['average_elapsed_time'] = merged_df.groupby('user_id')['elapsed_time'].transform('mean')\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "BHnuyZArD0Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_correctness = merged_df.groupby('user_id')['correctness_rate'].mean()\n",
        "user_correctness"
      ],
      "metadata": {
        "id": "gde8hzsED9Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_elapsed_time = merged_df.groupby('user_id')['average_elapsed_time'].mean()\n",
        "user_elapsed_time"
      ],
      "metadata": {
        "id": "_L1A5gOnD_px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_metrics_df = pd.DataFrame({'user_id': user_correctness.index,\n",
        "                                'correctness_rate': user_correctness.values,\n",
        "                                'user_elapsed_time': user_elapsed_time.values})"
      ],
      "metadata": {
        "id": "oKTCpDedEBQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the 80th percentile (quantile) of 'elapsed_time_scaled'\n",
        "top_20_percentile = user_metrics_df['user_elapsed_time'].quantile(0.3)\n",
        "below_20_percentile = user_metrics_df['user_elapsed_time'].quantile(0.7)\n",
        "\n",
        "high_achievers_threshold = 0.7\n",
        "struggling_learners_threshold = 0.3\n",
        "\n",
        "def segment_users(row):\n",
        "    if row['correctness_rate'] >= high_achievers_threshold and row['user_elapsed_time'] <= top_20_percentile:\n",
        "        return 'High Achievers'\n",
        "    elif row['correctness_rate'] <= struggling_learners_threshold and row['user_elapsed_time'] >= below_20_percentile:\n",
        "        return 'Struggling Learners'\n",
        "    else:\n",
        "        return 'Average Performers'\n",
        "\n",
        "user_metrics_df['user_segment'] = user_metrics_df.apply(segment_users, axis=1)\n"
      ],
      "metadata": {
        "id": "EYXyNvedETK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install surprise"
      ],
      "metadata": {
        "id": "5Ty_Vru-Eifj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "custom_dataset = Dataset.load_from_df(merged_df[['user_id', 'question_id', 'correctness_rate', 'average_elapsed_time']], reader)"
      ],
      "metadata": {
        "id": "K4IDLoXiEjwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, testset = train_test_split(custom_dataset, test_size=0.2)"
      ],
      "metadata": {
        "id": "zraEnikvEoS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import NMF\n",
        "\n",
        "# Choose the NMF algorithm\n",
        "algo = NMF()\n",
        "\n",
        "# Train the model on the training set\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict ratings for the test set\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)"
      ],
      "metadata": {
        "id": "je-7E8etEpn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import CoClustering\n",
        "\n",
        "# Choose the CoClustering algorithm\n",
        "algo = CoClustering()\n",
        "\n",
        "# Train the model on the training set\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict ratings for the test set\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)\n"
      ],
      "metadata": {
        "id": "9DGRqcgPEqAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import SVD\n",
        "\n",
        "# Choose the SVD algorithm\n",
        "algo = SVD()\n",
        "\n",
        "# Train the model on the training set\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict ratings for the test set\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)"
      ],
      "metadata": {
        "id": "owNuQicpEtCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 3\n",
        "user_segment = user_metrics_df[user_metrics_df['user_id'] == user_id]['user_segment'].values[0]\n",
        "\n",
        "if user_segment == 'High Achievers':\n",
        "    # Recommend challenging questions\n",
        "    recommended_questions = [(question_id, algo.predict(user_id, question_id).est) for question_id in range(trainset.n_items) if algo.predict(user_id, question_id).est <= 0.4]\n",
        "elif user_segment == 'Struggling Learners':\n",
        "    # Recommend moderately challenging questions\n",
        "    recommended_questions = [(question_id, algo.predict(user_id, question_id).est) for question_id in range(trainset.n_items) if algo.predict(user_id, question_id).est >= 0.6]\n",
        "else:\n",
        "    # Recommend easier questions or remedial content\n",
        "    recommended_questions = [(question_id, algo.predict(user_id, question_id).est) for question_id in range(trainset.n_items) if 0.4 < algo.predict(user_id, question_id).est < 0.6]\n",
        "\n",
        "# Sort recommended_questions based on the prediction score in descending order\n",
        "recommended_questions.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"Top 10 Detailed Recommendations for user {user_id}:\")\n",
        "for i, (question_id, prediction_score) in enumerate(recommended_questions[:10], 1):\n",
        "    print(f\"{i}. Question ID: {question_id}, Prediction Score: {prediction_score}\")\n"
      ],
      "metadata": {
        "id": "Rmxwh15uEvKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}